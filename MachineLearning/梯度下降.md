梯度下降是一种可用于尝试最小化任何函数的算法，例如具有两个以上参数的模型的成本函数：$J(w_1,w_2,...,w_n,b)$，那么我们的目标就是在参数$w_1$到$w_n$和$b$上最小化$J$（$\displaystyle\min_{w_1,...,w_n,b}J(w_1,w_2,...,w_n,b)$）

![](../image/MachineLearning/梯度下降-1.png)

## 梯度下降算法
$$
w=w-\alpha\frac{\partial}{\partial w}J(w,b)
$$
$$
b=b-\alpha\frac{\partial}{\partial b}J(w,b)
$$

$\alpha$：Learning rate，通常是0到1之间的一个小正数。alpha所做的是控制下坡步幅（正比关系）

$\frac{\partial}{\partial w}J(w,b)$：Derivative，朝哪个方向迈出一小步。