# 3 Netty入门应用
## 3.1 Netty开发环境搭建

## 3.2 Netty服务端开发
```java
package com.airing.netty.time;

import io.netty.bootstrap.ServerBootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.ChannelOption;
import io.netty.channel.EventLoopGroup;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.SocketChannel;
import io.netty.channel.socket.nio.NioServerSocketChannel;

public class TimeServer {
    public void bind(int port) throws Exception {
        EventLoopGroup bossGroup = new NioEventLoopGroup();
        EventLoopGroup workerGroup = new NioEventLoopGroup();
        try {
            ServerBootstrap serverBootstrap = new ServerBootstrap();
            serverBootstrap.group(bossGroup, workerGroup)
                    .channel(NioServerSocketChannel.class)
                    .option(ChannelOption.SO_BACKLOG, 1024)
                    .childHandler(new ChildChannelHandler());
            ChannelFuture channelFuture = serverBootstrap.bind(port).sync();
            channelFuture.channel().closeFuture().sync();
        } catch (Exception e) {

        } finally {
            bossGroup.shutdownGracefully();
            workerGroup.shutdownGracefully();
        }
    }

    private class ChildChannelHandler extends ChannelInitializer<SocketChannel> {
        @Override
        protected void initChannel(SocketChannel socketChannel) throws Exception {
            socketChannel.pipeline().addLast(new TimeServerHandler());
        }
    }

    public static void main(String[] args) throws Exception {
        new TimeServer().bind(8888);
    }
}
```
```java
package com.airing.netty.time;

import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;
import java.text.SimpleDateFormat;
import java.util.Date;

public class TimeServerHandler extends ChannelInboundHandlerAdapter {
    private static final SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        ByteBuf buf = (ByteBuf) msg;
        byte[] bytes = new byte[buf.readableBytes()];
        buf.readBytes(bytes);
        String content = new String(bytes, "utf-8");
        System.out.println("request content: " + content);
        String response = "query time".equals(content) ? sdf.format(new Date()) : "request error";
        ByteBuf respBuf = Unpooled.copiedBuffer(response.getBytes("utf-8"));
        ctx.write(respBuf);
    }

    @Override
    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {
        ctx.flush();
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        ctx.close();
    }
}
```

## 3.3 Netty客户端开发
```java
package com.airing.netty.time;

import io.netty.bootstrap.Bootstrap;
import io.netty.channel.ChannelFuture;
import io.netty.channel.ChannelInitializer;
import io.netty.channel.ChannelOption;
import io.netty.channel.nio.NioEventLoopGroup;
import io.netty.channel.socket.SocketChannel;
import io.netty.channel.socket.nio.NioSocketChannel;

public class TimeClient {
    private void connect(String host, int port) {
        NioEventLoopGroup eventLoopGroup = new NioEventLoopGroup();
        try {
            Bootstrap bootstrap = new Bootstrap();
            bootstrap.group(eventLoopGroup)
                    .channel(NioSocketChannel.class)
                    .option(ChannelOption.TCP_NODELAY, true)
                    .handler(new ChannelInitializer<SocketChannel>() {
                        @Override
                        protected void initChannel(SocketChannel socketChannel) throws Exception {
                            socketChannel.pipeline().addLast(new TimeClientHandler());
                        }
                    });
            ChannelFuture channelFuture = bootstrap.connect(host, port).sync();
            channelFuture.channel().closeFuture().sync();
        } catch (Exception e) {

        } finally {
            eventLoopGroup.shutdownGracefully();
        }
    }

    public static void main(String[] args) {
        new TimeClient().connect("127.0.0.1", 8888);
    }
}
```
```java
package com.airing.netty.time;

import io.netty.buffer.ByteBuf;
import io.netty.buffer.Unpooled;
import io.netty.channel.ChannelHandlerContext;
import io.netty.channel.ChannelInboundHandlerAdapter;

public class TimeClientHandler extends ChannelInboundHandlerAdapter {
    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception {
        String reqContent = "query time";
        byte[] reqBytes = reqContent.getBytes("utf-8");
        ByteBuf reqBuf = Unpooled.buffer(reqBytes.length);
        reqBuf.writeBytes(reqBytes);
        ctx.writeAndFlush(reqBuf);
    }

    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception {
        ByteBuf response = (ByteBuf) msg;
        byte[] respBytes = new byte[response.readableBytes()];
        response.readBytes(respBytes);
        System.out.println(new String(respBytes, "utf-8"));
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {
        ctx.close();
    }
}
```

## 3.4 运行和调试

# 4 TCP粘包/拆包问题的解决之道
## 4.1 TCP粘包/拆包
TCP是一个“流”协议，所谓流，就是没有界限的一串数据。TCP底层并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行包的拆分，所以在业务上认为，一个完整的包可能会被TCP拆分成多个包进行发送，也有可能把多个小的数据包封装成一个大的数据包进行发送，这就是TCP的粘包/拆包问题。

### 4.1.1 TCP粘包/拆包问题说明
CLIENT ------|D2|------|D1|------SERVER  
包含但不限于以下5种情况：
- CLIENT ------|D2|------|D1|------SERVER
- CLIENT ------|D2-D1|------SERVER
- CLIENT ------|D2_2|------|D2_1-D1|------SERVER
- CLIENT ------|D2-D1_2|------|D1_1|------SERVER
- CLIENT ------|D2_2|------|D2_1|------|D1_2|------|D1_1|------SERVER

### 4.1.2 TCP粘包/拆包发生的原因
问题发生原因有三个，分别如下：
1. 应用程序write写入的字节大小大于套接字接口发送缓冲区大小
2. 进行MSS（最大报文段长度）大小的TCP分段
3. 以太网帧的payload大于MTU（最大传输单元）进行IP分片

> MSS（Maximum Segment Size）是指在 TCP 通信中，每个TCP报文段中的最大数据部分（payload）的长度。它定义了每个TCP报文段可以携带的有效数据的最大尺寸。MSS由网络设备（如路由器、防火墙）或操作系统在建立TCP连接时协商确定的。在TCP三次握手过程中，双方会交换各自支持的MSS值，并根据这些值来选择合适的MSS作为通信的上限。通常是通过IP层的最大传输单元（MTU）和TCP/IP协议头的开销来决定的。MTU是指网络链路层（如以太网）可以传输的最大数据包大小。而MSS应该略小于MTU，以容纳TCP/IP协议头的额外开销。
> 
> 常见的MSS值为1460字节，在以太网上使用1500字节的MTU情况下，减去TCP/IP协议头的20字节和IP协议头的20字节。但是具体的MSS值可能因为网络设备、操作系统或配置而有所不同。控制MSS的大小对于网络性能和传输效率非常重要。较大的MSS可以提高数据传输的效率，因为每个报文段可以携带更多的有效数据。然而，如果MSS过大超过了某些网络链路的MTU限制，会导致数据包分片（fragmentation），增加网络开销和延迟。

### 4.1.3 粘包问题的解决策略
由于底层的TCP无法理解上层的业务数据，所以在底层是无法保证数据包不被拆分和重组的，这个问题只能通过上层的应用协议栈来解决，主要有以下方法：
1. 消息定长，空位补空格
2. 在包尾增加回车换行符进行分隔，例如FTP协议
3. 将消息分为消息头和消息体，消息头中包含表示消息体总长度的字段
4. 更复杂的应用层协议

## 4.2 未考虑TCP粘包导致功能异常的案例

## 4.3 利用LineBaseFrameDecoder解决TCP粘包问题

### 4.3.4 LineBaseFrameDecoder和StringDecoder的原理分析
LineBaseFrameDecoder的工作原理是它依次遍历ByteBuf中的可读字节，判断是否有“\n”或者“\r\n”，如果有，就以此位置为结束位置，从可读索引到结束位置区间的字节就组成了一行。**它是以换行符为结束标志的解码器**，支持携带结束符和不携带结束符两种解码方式，**同时支持配置单行的最大长度**。如果连续读取到最大长度后仍然没有发现换行符，就会抛出异常，同时忽略掉之前读取到的异常码流。  
StringDecoder的功能非常简单，就是将接收到的对象转换成字符串，然后继续调用后面Handler。

# 5 分隔符和定长解码器的应用
TCP以流的方式进行数据传输，上层的应用协议为了对消息进行区分，往往采用如下4种方式。
1. 消息长度固定，累积读取到长度总和为定长的报文后，就认为读到了一个完整的消息。
2. 将回车换行符作为消息的结束标志。
3. 将特殊的分隔符作为消息的结束标志。
4. 通过在消息头中定义一个字段来标识消息的总长度。

另外两种实用的解码器：
1. DelimiterBasedFrameDecoder：自动完成以分隔符作为结束标志的消息的解码
2. FixedLengthFrameDecoder：自动完成对定长消息的解码

## 5.1 DelimiterBasedFrameDecoder应用开发

## 5.2 FixedLengthFrameDecoder应用开发




















